\documentclass[11pt]{article}

\usepackage{amsmath,amsthm}
%\usepackage{amssymb}
%%%%% Matrix stretcher
% use it as:
%\begin{pmatrix}[1.5]
% ...
\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
  \edef\arraystretch{#1}%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols c}}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\extrafootertext[1]{%
    \bgroup
    \renewcommand\thefootnote{\fnsymbol{footnote}}%
    \renewcommand\thempfootnote{\fnsymbol{mpfootnote}}%
    \footnotetext[0]{#1}%
    \egroup
}


%%%%%%%%%%%%% Colors %%%%%%%%%%%%%
\usepackage[dvipsnames]{xcolor}

\definecolor{C0}{HTML}{1d1d1d}
\definecolor{C1}{HTML}{1e3668}
\definecolor{C2}{HTML}{199d8b}
\definecolor{C3}{HTML}{d52f4c}
\definecolor{C4}{HTML}{5ab2d6}
\definecolor{C5}{HTML}{ffb268}
\definecolor{C6}{HTML}{ff7300} % for commenting - {fire orange}dd571c
\definecolor{C7}{HTML}{777b7e} % for remarks - {steel grey}
\color{C0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%% Fonts %%%%%%%%%%%%% 
%\usepackage{fontspec}
\usepackage[no-math]{fontspec} % for text
\usepackage{unicode-math}        % for math

\emergencystretch=8pt
\hyphenpenalty=1000 % default 50
\tolerance=800      % default 200
%\righthyphenmin=4
%\lefthyphenmin=4

%%% Text Font: Vollkorn + Math Font: Latin Modern (default) %%%
\setmainfont{Vollkorn}[
UprightFont = Vollkorn-Regular,
ItalicFont =Vollkorn-Italic, 
BoldItalicFont={Vollkorn-BoldItalic},
BoldFont = Vollkorn-Bold,
RawFeature=+lnum,
WordSpace=1.7,
] 

%%% Text Font: Palatino + Math Font: Asana-Math %%%
%\setmainfont{Palatino}[
%BoldFont = Palatino-Bold,
%ItalicFont = Palatino-Italic,
%BoldItalicFont={Palatino-BoldItalic},
%RawFeature=+lnum,
%WordSpace=1.7,
%]
%\setmathfont{asana-math}

%%% Text Font: Arno Pro + Math Font: Minion Pro %%%
%\setmainfont{Arno Pro}[
%UprightFont = *-Regular,
%ItalicFont = Vollkorn-Italic, 
%BoldItalicFont={*-BoldItalic},
%BoldFont = *-Bold,
%RawFeature=+lnum,
%WordSpace=1.7,
%Scale= 1.1
%] 
% Minion Pro is too expensive

%%% Math Fonts %%%
%\setmathfont{Vollkorn}
%\setmathfont{Latin Modern Math}
%\setmathfont{TeX Gyre Pagella Math}
%\setmathfont{TeX Gyre Termes Math}
%\setmathfont{TeX Gyre DejaVu Math}
%\setmathfont[Scale=MatchLowercase]{DejaVu Math TeX Gyre}
%\setmathfont{XITS Math}
%\setmathfont{Libertinus Math}
%\setmathfont[Scale=MatchUppercase]{Asana Math}
%\setmathfont{STIX Two Math}

%\usepackage{kpfonts-otf}
%\setmathfont{KpMath-Regular.otf}[version=regular]
%\setmathfont{KpMath-Bold.otf}[version=bold]
%\setmathfont{KpMath-Semibold.otf}[version=semibold]
%\setmathfont{KpMath-Sans.otf}[version=sans]
%\setmathfont{KpMath-Light.otf}[version=light]


%%% CJK Fonts %%%
\usepackage[scale=.78]{luatexja-fontspec}
\setmainjfont{BabelStone Han}[AutoFakeBold]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% This package simplifies the insertion of external multi-page PDF or PS documents.
\usepackage{pdfpages}

% cref
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=C4,
    filecolor=magenta,      
    urlcolor=cyan,
    }

\usepackage[nameinlink,noabbrev,capitalize]{cleveref}
% \crefname{ineq}{}{}
% \crefname{equation}{}{}
% \creflabelformat{ineq}{#2{\textup{(1)}}#3}
% \creflabelformat{equation}{#2\textup{(#1)}#3}

%%%%%%%%%%%%% Environments %%%%%%%%%%%%%%%%
%amsthm has three separate predefined styles:	
%
%\theoremstyle{plain} is the default. it sets the text in italic and adds extra space above and below the \newtheorems listed below it in the input. it is recommended for theorems, corollaries, lemmas, propositions, conjectures, criteria, and (possibly; depends on the subject area) algorithms.
%
%\theoremstyle{definition} adds extra space above and below, but sets the text in roman. it is recommended for definitions, conditions, problems, and examples; i've alse seen it used for exercises.
%
%\theoremstyle{remark} is set in roman, with no additional space above or below. it is recommended for remarks, notes, notation, claims, summaries, acknowledgments, cases, and conclusions.

%%%  theorem-like environment %%%
\theoremstyle{plain} % default theorem style
\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{property}[theorem]{Property}

\newtheorem{definition}[theorem]{Definition}

%%% definition-like environment %%%
%\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{problem}[theorem]{Problem}


%%% framed package is great %%%
\usepackage{framed}
\newenvironment{solution}
{\color{C2}\normalfont\begin{framed}\begingroup\textbf{Solution:} }
  {\endgroup\end{framed}}

\newtheoremstyle{remark}% name of the style to be used
  {}% measure of space to leave above the theorem. E.g.: 3pt
  {}% measure of space to leave below the theorem. E.g.: 3pt
  {\color{C3}}% name of font to use in the body of the theorem
  {}% measure of space to indent
  {\color{C3}\bfseries}% name of head font
  {.}% punctuation between head and body
  { }% space after theorem head; " " = normal interword space
  {}
\theoremstyle{remark}
\newtheorem{remarkx}[theorem]{Remark}
\newenvironment{remark}
  {\pushQED{\qed}\renewcommand{\qedsymbol}{$\triangle$}\remarkx}
  {\popQED\endremarkx}
  
\newenvironment{point}
  {\O~~}
  {}

\usepackage{thmtools}
\usepackage{thm-restate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% This package is for the long equal sign \xlongequal{}
\usepackage{extarrows}


% Page Formatting
\usepackage[
    paper=a3paper,
    inner=22mm,         % Inner margin
    outer=22mm,         % Outer margin
    bindingoffset=0mm, % Binding offset
    top=28mm,           % Top margin
    bottom=22mm,        % Bottom margin
    %showframe,         % show how the type block is set on the page
]{geometry}

\setlength{\parindent}{0em}
\setlength{\parskip}{.7em}


\usepackage{tikz}
\usepackage{graphicx}
\usepackage{enumitem}
\setlist{topsep=0pt}

\usepackage{bm}

\usepackage[font=scriptsize,labelfont=bf]{caption}
\usepackage{listings}
\lstset{basicstyle=\ttfamily,breaklines=true}
% \setlength{\parskip}{1em}
% \setlength{\parindent}{0em}
\usepackage{dsfont}
\newcommand{\bOne}{\mathds{1}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\CoV}{\operatorname{Co\mathbb{V}}}

% header
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\small   \bfseries Homework}
\fancyhead[C]{\small   \bfseries Fall 2023}
\fancyhead[R]{\small   \bfseries Zhou}


\begin{document}

\begin{center}
    \text{\Large{Regression and SVD
}}

    {\text{Kaiwen Zhou}}
\end{center}
\vspace{2em}

\tableofcontents

\section{Topic: Regression and SVD}
\begin{problem}
    Show that $\operatorname{rank}(X)=\operatorname{rank}\left(X^\top X\right)$ for any matrix $X$ with real entries.
\end{problem}
\begin{solution}

\textbf{Method 1:} Suppose $X\in \mathbb{R}^{m\times n}$. Then, by SVD, we have $X = U\Sigma V^\top$, where $U\in \mathbb{R}^{m\times m}$, $V\in \mathbb{R}^{n\times n}$ are orthogonal matrices, and $\Sigma \in \mathbb{R}^{m\times n}$ having non-negative singular values on its diagonal and zeroes elsewhere.

    Since $U, V$ are orthogonal, they are invertible. Because multiplying an invertible matrix does not change the rank of a matrix, we get 
    $\operatorname{rank}(X) = \operatorname{rank}(U\Sigma V^\top) =\operatorname{rank}(\Sigma)$.

    On the other hand, we have
    $$
    \operatorname{rank}(X^\top X) = \operatorname{rank}( V\Sigma^\top U^\top U\Sigma V^\top) =\operatorname{rank}(V\Sigma^\top \Sigma V^\top)=\operatorname{rank}(\Sigma^\top \Sigma) = \operatorname{rank}(\Sigma)
    $$
    Hence, we have $\operatorname{rank}(X)=\operatorname{rank}(X^\top X)$.
\vspace{1em}
\hrule
\textbf{Method 2:} We will prove $Im(X^\top) = Im(X^\top X)$. First, we have
$$
Im(X^\top X) = \{X^\top Xv \mid v\in \mathbb{R}^{n}\} =  \{X^\top u \mid u\in Im(X)\}
$$
By the rank-nullity theorem, we get $dim(Im(X)) + dim(Ker(X^\top)) = m$. Moreover, $Im(X)$ and $Ker(X^\top)$ are orthogonal. Therefore $Im(X)$ and $Ker(X^\top)$ together span the whole space $\mathbb{R}^m$. 

It follows that $\forall u \in \mathbb{R}^m$, we can uniquely decompose it as the sum $u = u_{r} + u_{n}$ where $u_r\in Im(X)$ and $u_n \in Ker(X^\top)$.

Therefore, we obtain
$$
Im(X^\top)  =  \{X^\top u, u\in \mathbb{R}^m\} = \{X^\top (u_r + u_{n})\mid u_r\in Im(X),u_n \in Ker(X^\top)\} = \{X^\top u_r\mid u_r\in Im(X)\} = Im(X^\top X)
$$
Hence, we conclude
$$
\operatorname{rank}(X) = dim(Im(X)) = dim(Im(X^\top)) = dim(Im(X^\top X)) = \operatorname{rank}(X^\top X)
$$
\end{solution}


\begin{problem}
    Write a numpy function to compute the pseudo-inverse of a real matrix, building upon the numpy SVD function discussed in class. Test that your function works by generating a sequence of random invertible matrices, and check that for each one, your pseudo-inverse equals the actual inverse up to numerical precision.
\end{problem}

\begin{solution}
    WLOG, suppose $X\in \mathbb{R}^{m\times n}$, $m\le n$. Then, by SVD, we have $X = U\Sigma V^\top$, where $U\in \mathbb{R}^{m\times m}$, $V\in \mathbb{R}^{n\times n}$ are orthogonal matrices, and $\Sigma \in \mathbb{R}^{m\times n}$ has non-negative singular values $\sigma_1\ge \cdots \ge \sigma_p \ge \sigma_{p+1} = \cdots = \sigma_m = 0$, $p\le \min\{m, n\}$, on its diagonal and zeroes elsewhere.

    Then, we have $$
    X^{\dagger} = \left(X^\top X\right)^{-1} X^\top = \left(V\Sigma^\top U^\top U \Sigma V^\top\right)^{-1} V\Sigma^\top U^\top = \left(V \left(\Sigma^\top \Sigma\right) V^\top \right)^{-1} V\Sigma^\top U^\top = V \left(\Sigma^\top \Sigma\right)^{-1}\Sigma^\top U^\top  = V \Sigma^{\dagger} U^\top
    $$
    That is, the pseudo-inverse given by the SVD is $X^{\dagger} = V \Sigma^{\dagger} U^\top$.

    For the code, see the attached jupyter notebook.
\end{solution}


\begin{problem}
    Prove that the Moore-Penrose pseudo-inverse is given as a one-sided limit of ridge regression problems, i.e. prove
$$
X^\dagger y=\lim _{\lambda \rightarrow 0^{+}}\left(X^\top X+\lambda I\right)^{-1} X^\top y
$$
Hint: use the SVD.
\end{problem}
\begin{solution}
    Recall that an alternate definition of $X^\dagger$, see \cite{albert1972} Albert (1972), is 
    {\color{C6}\begin{framed}{
        \begin{definition}
    The Moore-Penrose pseudo-inverse $X^\dagger$is defined so that $X^\dagger y$ is the minimum-norm vector among all minimizers of $\min_{\beta}\|y-X \beta\|^2$.
    \end{definition}}\end{framed}}

    Therefore, we want to show the limit we get from the RHS is a minimizer of $\min_{\beta}\|y-X \beta\|^2$, and it's the one with the smallest norm among all minimizers. We apply the SVD to help us show this. 

    WLOG, suppose $X\in \mathbb{R}^{m\times n}$, $m\le n$. Then, by SVD, we have $X = U\Sigma V^\top$, where $U\in \mathbb{R}^{m\times m}$, $V\in \mathbb{R}^{n\times n}$ are orthogonal matrices, and $\Sigma \in \mathbb{R}^{m\times n}$ has non-negative singular values $\sigma_1\ge \cdots \ge \sigma_p \ge \sigma_{p+1} = \cdots = \sigma_m = 0$, $p\le \min\{m, n\}$, on its diagonal and zeroes elsewhere.

    Then, we have \begin{align*}
    \left(X^\top X+\lambda I\right)^{-1} X^\top y &= \left(V\Sigma^\top U^\top U \Sigma V^\top +\lambda I\right)^{-1} V\Sigma^\top U^\top y\\
    &= \left(V \left(\Sigma^\top \Sigma +\lambda I \right) V^\top \right)^{-1} V\Sigma^\top U^\top y\\
    &= V \left(\Sigma^\top \Sigma +\lambda I \right)^{-1}\Sigma^\top U^\top y\\
    &=V \begin{bmatrix}
        \frac{\sigma_1}{\lambda + \sigma_1^2} & & & &\\
        & \ddots &  &  &\\
         & & \frac{\sigma_p}{\lambda + \sigma_p^2} & &\\
         & &  & &\\
         &  &  &  &\\
         &  &  &  &\\
    \end{bmatrix} U^\top y 
    \end{align*}
    We then take the limit and obtain
    $$
    X^\dagger y = \lim _{\lambda \rightarrow 0^{+}}\left(X^\top X+\lambda I\right)^{-1} X^\top y = \lim _{\lambda \rightarrow 0^{+}} V \begin{bmatrix}
        \frac{\sigma_1}{\lambda + \sigma_1^2} & & & &\\
        & \ddots &  &  &\\
         & & \frac{\sigma_p}{\lambda + \sigma_p^2} & &\\
         & &  & &\\
         &  &  &  &\\
         &  &  &  &\\
    \end{bmatrix} U^\top y = V \begin{bmatrix}
        \frac{1}{\sigma_1} & & & &\\
        & \ddots &  &  &\\
         & & \frac{1}{\sigma_p} & &\\
         & &  & &\\
         &  &  &  &\\
         &  &  &  &\\
    \end{bmatrix} U^\top y := V S U^\top y, \quad S \in \mathbb{R}^{n\times m}
    $$

    To be a minimizer of $\min_{\beta}\|y-X \beta\|^2$, we need $\beta^* = X^\dagger y = V S U^\top y$ to satisfy the first-order necessary condition
    \begin{align}
        \frac{\partial L(\beta)}{\partial \beta} = \frac{\partial}{\partial \beta} \frac{1}{2}\|y-X \beta\|^2 = \frac{\partial}{\partial \beta} \frac{1}{2} (y-X \beta)^\top (y-X \beta) = X^\top X \beta - X^\top y = 0 \label{eq:first-order nece}
    \end{align}
    Plug $\beta^* = V S U^\top y$ in \cref{eq:first-order nece} and apply SVD on $X$, we get
    \begin{align*}
    X^\top X \beta^* - X^\top y &= V\Sigma^\top U^\top U\Sigma V^\top V S U^\top y - V\Sigma^\top U^\top y\\
    U, V \text{orthogonal}\Longrightarrow 
    &=V\Sigma^\top \Sigma  S U^\top y - V\Sigma^\top U^\top y \\
    &= V\begin{bmatrix}
        \sigma_1 & & & &\\
        & \ddots &  &  &\\
         & & \sigma_p & &\\
         & &  & &\\
         &  &  &  &\\
         &  &  &  &\\
    \end{bmatrix} \begin{bmatrix}
        \sigma_1 & & & & & &\\
        & \ddots & & & & &\\
         & & \sigma_p & & & &\\
         & &  & & & & \\
         & &  & & & & 
    \end{bmatrix}  \begin{bmatrix}
        \frac{1}{\sigma_1} & & & &\\
        & \ddots &  &  &\\
         & & \frac{1}{\sigma_p} & &\\
         & &  & &\\
         &  &  &  &\\
         &  &  &  &\\
    \end{bmatrix} U^\top y - V\Sigma^\top U^\top y\\
    &= V\Sigma^\top  U^\top y - V\Sigma^\top U^\top y \\
    &= 0
    \end{align*}

    Therefore, $\beta^* = X^\dagger y = V S U^\top y$ is a minimizer of $\min_{\beta}\|y-X \beta\|^2$.

    We now assume that there exists another minimizer $\beta_1 \in \mathbb{R}^n$ other than $\beta^*$ and that $y = X\beta_1 + \varepsilon$ where $\varepsilon\in \mathbb{R}^m$.

    Then, by \cref{eq:first-order nece}, we have 
    $$
    0 = X^\top X \beta_1 - X^\top y = X^\top X \beta_1 - X^\top (X\beta_1+\epsilon) \Longrightarrow 0 = X^\top \varepsilon = V\Sigma^\top U^\top \varepsilon \Longrightarrow 0 = \Sigma^\top U^\top \varepsilon  = \begin{bmatrix}
        \sigma_1 u_1^\top \varepsilon \\
        \vdots\\
         \sigma_p u_p^\top \varepsilon\\
         0\\
         \vdots\\
         0
    \end{bmatrix} \Longrightarrow u_i^\top \varepsilon = 0, \text{ for } i=1, \ldots, p
    $$
    It follows that
    $$
    SU^\top \varepsilon = \begin{bmatrix}
        \frac{1}{\sigma_1} & & & &\\
        & \ddots &  &  &\\
         & & \frac{1}{\sigma_p} & &\\
         & &  & &\\
         &  &  &  &\\
         &  &  &  &\\
    \end{bmatrix} U^\top \varepsilon = \begin{bmatrix}
        \frac{1}{\sigma_1} u_1^\top \varepsilon \\
        \vdots\\
         \frac{1}{\sigma_p} u_p^\top \varepsilon\\
         0\\
         \vdots\\
         0
    \end{bmatrix} = 0
    $$
    Finally, we consider $\beta^{*^\top}\beta^*$ and obtain
    \begin{align*}
        \|\beta^* \|^2=\beta^{*^\top}\beta^* 
        &= (V S U^\top y)^\top V S U^\top y\\
        &= (V S U^\top X\beta_1 + V S U^\top\varepsilon)^\top  (V S U^\top X\beta_1 + V S U^\top\varepsilon)\\
        SU^\top \varepsilon=0\Longrightarrow&= (V S \Sigma V^\top \beta_1)^\top  (V S \Sigma V^\top \beta_1 )\\
        &= \beta_1^\top V \Sigma^\top S^\top V^\top VS\Sigma V^\top \beta_1\\
        V \text{ orthogonal } \Longrightarrow
        &= \beta_1^\top V \Sigma^\top S^\top S\Sigma V^\top \beta_1\\
        &=\beta_1^\top V\begin{bmatrix}
        1 & &  & & &\\
        & \ddots &  &  & &\\
         & & 1 & & & \\
         & &  & 0 & & \\
         &  &  &  & \ddots & \\
         &  &  &  & &  0\\
    \end{bmatrix}^\top\begin{bmatrix}
        1 & &  & & &\\
        & \ddots &  & & & \\
         & & 1 & & & \\
         & &  & 0 & & \\
         &  &  &  & \ddots & \\
         &  &  &  & &  0\\
    \end{bmatrix}V^\top \beta_1\\
    &= \beta_1^\top \left(\sum_{i=1}^p v_iv_i^\top\right) \beta_1\\
    &= \beta_1^\top \left(\sum_{i=1}^n v_iv_i^\top\right) \beta_1 - \beta_1^\top \left(\sum_{i=p+1}^n v_iv_i^\top\right) \beta_1\\
    &= \beta_1^\top VV^\top \beta_1 - \sum_{i=p+1}^n\beta_1^\top v_iv_i^\top\beta_1\\
    &= \|\beta_1\|^2 - \sum_{i=p+1}^n \|v_i^\top\beta_1\|^2\\
    &\le \|\beta_1\|^2
    \end{align*}
    where the equality is attained when $p = n$, i.e., $X$ must be full column ranked.
    
    Since $\beta_1$ is chosen arbitrarily,  we can now conclude that $\beta^* = X^\dagger y = V S U^\top y$ is a minimizer of $\min_{\beta}\|y-X \beta\|^2$, and it's the one with the smallest norm among all minimizers.
    
    Hence, we conclude that the limit given by $X^\dagger y=\lim _{\lambda \rightarrow 0^{+}}\left(X^\top X+\lambda I\right)^{-1} X^\top y$ is, by definition, the Moore-Penrose pseudo-inverse.
\end{solution}


\begin{thebibliography}{1}

\bibitem{albert1972}
Albert, Arthur (1972).
\textit{Regression and the Moore-Penrose pseudo-inverse}. 
Academic Press.

\end{thebibliography}

\end{document}

